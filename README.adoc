= Machine Learning
:encoding: utf-8
:lang: en
:layout: docs
:toc:
:toclevels: 6
:toc-placement!:
:nofooter:

toc::[]

== Classification: Predict category of new observation
=== Performance measure: confusion matrix

.Confusion matrix
[options="header,footer"]
|===
|||Prediction|Prediction
|||P|N
|Truth|P|TP|FN
|Truth|N|FP|TN
|===

. Accuracy: `(TP+TN)/(TP+FP+FN+TN)`
. Precision: `TP/(TP+FP)`
. Recall: `TP/(TP+FN)`

----
Accuracy = correctly classified instances / total amount of classified instances
Error = 1 - Accuracy
----

=== Decision tree
Goal: end up with pure leafs — leafs that contain observations of one particular class. At each node iterate over different feature tests and choose the best. Use **spliting criteria** to decide which test to use. The best test leads to nicely divided classes -> high information gain

**Pruning** tree restricts node size = higher bias = lower overfit chance.

=== kNN
Predict using the comparison of **unseen data** and the **training set**

=== ROC Curve
Receiver Operator Characteristic Curve: Binary classification which uses decision trees and k-NN to predict class, giving probability as output

----
# TPR = True positive rate; FPR = False positive rate
TPR = recall
FPR = FP / (FP+TN)
----





== Regression
=== Performance measure
. Root Mean Squared Error (RMSE)
. R-squared
. Adjusted R-squared: penalizes more predictors
. p-values: low p-values = parameter has significant influence

=== Linear model
. Simple linear: 1 predictor (with approximately linear relationship) to model the response
. Multi-linear: Higher **predictive power** and **accuracy** = Lower RMSE and higher R-squared
. Ridge Regression is a technique for analyzing multiple regression data that suffer from multicollinearity.
    - It reduces the standard errors.
    - It adds penalty equivalent to square of the magnitude of coefficients

=== Techniques for non-parametric regression
. KNN
. Kernel Regression
. Regression Trees





== Clustering
- Grouping objects in clusters

=== Performance measure
. Within sum of squares (WSS)
. Between cluster sum of squares (BSS)
. Dunn’s index

=== k-Means
- Partition data in **k disjoint** subsets

=== Scree plot: Choosing k
- WSS keeps decreasing as k increases -> Find k that minimizes WSS

----
TSS = WSS + BSS
WSS / TSS < 0.2
----

=== Hierarchical clustering
. Simple-Linkage: minimal distance between clusters = low BSS
. Complete-Linkage: maximal distance between clusters = high BSS
. Average-Linkage: average distance between clusters





== Bias and variance
- Prediction error = reducible + irreducible error
- Reducible error = Bias & Variance

----
Error due to bias = Wrong assumption = diff(prediction, truth) = complexity of model
More model restrictions = high bias = low variance = underfitting
----

----
Error due to variance = error due to the sampling of the training set
Model fits training set closely = high variance = low bias = overfitting
----





== Model
=== Cross validation
- Use to validate the stability of the model

== References
- https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463
